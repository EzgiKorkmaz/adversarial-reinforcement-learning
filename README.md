# Adversarial Reinforcement Learning



A curated reading list for the adversarial perspective in deep reinforcement learning. The list covers topics ranging from adversarial attacks on deep reinforcement learning policies to adversarial training techniques and interpretability in deep reinforcement learning robustness to adversarial state detection algortihms for robust decision making.

Delving Into Adversarial Attacks on Deep Policies. ICLR Workshop 2017. [[Link]](https://arxiv.org/abs/1705.06452)

Adversarial Attacks on Neural Network Policies. ICLR Workshop 2017. [[Link]](https://openreview.net/pdf?id=ryvlRyBKl)

Robust Adversarial Reinforcement Learning. ICML 2017. [[Link]](http://proceedings.mlr.press/v70/pinto17a/pinto17a.pdf)

Adversarial Policies: Attacking Deep Reinforcement Learning. ICLR 2020. [[Link]](https://openreview.net/pdf?id=HJgEMpVFwB)

Stealthy and Efficient Adversarial Attacks Against Deep Reinforcement Learning. AAAI 2020. [[Link]](https://ojs.aaai.org/index.php/AAAI/article/view/6047/5903)

Nesterov Momentum Adversarial Perturbations in the Deep Reinforcement Learning Domain. ICML Workshop 2020. [[Link]](https://biases-invariances-generalization.github.io/pdf/big_33.pdf)

Investigating Vulnerabilities of Deep Neural Policies. UAI 2021. [[Link]](https://proceedings.mlr.press/v161/korkmaz21a.html)

Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs. AAAI 2022. [[Link]](https://aaai.org/papers/07229-deep-reinforcement-learning-policies-learn-shared-adversarial-features-across-mdps/)

Adversarial Robust Deep Reinforcement Learning Requires Redefining Robustness. AAAI 2023. [[Link]](https://ojs.aaai.org/index.php/AAAI/article/view/26009)

Detecting Adversarial Directions in Deep Reinforcement Learning to Make Robust Decisions. ICML 2023. [[Link]](https://proceedings.mlr.press/v202/korkmaz23a.html)

Understanding and Diagnosing Deep Reinforcement Learning. ICML 2024. [[Link]](https://openreview.net/pdf?id=s9RKqT7jVM)

AI Safety: From Reinforcement Learning to Foundation Models, AAAI 2025. [[Link]](https://sites.google.com/view/aisafety-aaai2025)
